{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD5 Hash comparison on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hashes(image_folder, dataset_name):\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "    hashes = []\n",
    "    for root, dirs, files in os.walk(image_folder):\n",
    "        for file_name in files:\n",
    "            if os.path.splitext(file_name)[1].lower() in image_extensions:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                try:\n",
    "                    # reading the file in binary mode and computing MD5 hash\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        file_data = f.read()\n",
    "                        file_hash = hashlib.md5(file_data).hexdigest()\n",
    "                    subfolder = os.path.relpath(root, image_folder)\n",
    "                    hashes.append({\n",
    "                        \"Dataset\": dataset_name,\n",
    "                        \"Subfolder\": subfolder,\n",
    "                        \"FilePath\": file_path,\n",
    "                        \"FileName\": file_name,\n",
    "                        \"FileHash\": file_hash\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pairs = [\n",
    "    {\n",
    "        \"Duplicate\": {\n",
    "            \"Path\": \"../data/duplicate_data/ISIC-2017-1-FOLD\",\n",
    "            \"Name\": \"ISIC-2017-1-FOLD\"\n",
    "        },\n",
    "        \"Original\": {\n",
    "            \"Path\": \"../data/original_data/ISIC-2017-Challenge\",\n",
    "            \"Name\": \"ISIC-2017-Challenge\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"Duplicate\": {\n",
    "            \"Path\": \"../data/duplicate_data/ISIC-2018\",\n",
    "            \"Name\": \"ISIC-2018\"\n",
    "        },\n",
    "        \"Original\": {\n",
    "            \"Path\": \"../data/original_data/ISIC-2018-Challenge\",\n",
    "            \"Name\": \"ISIC-2018-Challenge\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"Duplicate\": {\n",
    "            \"Path\": \"../data/duplicate_data/ISIC-2019-Preprocessed-Dataset\",\n",
    "            \"Name\": \"ISIC-2019-Preprocessed-Dataset\"\n",
    "        },\n",
    "        \"Original\": {\n",
    "            \"Path\": \"../data/original_data/ISIC-2019-Challenge\",\n",
    "            \"Name\": \"ISIC-2019-Challenge\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hashes for ISIC-2017-1-FOLD dataset ...\n",
      "Computing hashes for ISIC-2017-Challenge dataset ...\n",
      "Computing hashes for ISIC-2018 dataset ...\n",
      "Computing hashes for ISIC-2018-Challenge dataset ...\n",
      "Computing hashes for ISIC-2019-Preprocessed-Dataset dataset ...\n",
      "Computing hashes for ISIC-2019-Challenge dataset ...\n"
     ]
    }
   ],
   "source": [
    "# computing hashes for all datasets\n",
    "all_hashes = []\n",
    "for pair in dataset_pairs:\n",
    "    for dataset_type in [\"Duplicate\", \"Original\"]:\n",
    "        dataset_info = pair[dataset_type]\n",
    "        dataset_path = dataset_info[\"Path\"]\n",
    "        dataset_name = dataset_info[\"Name\"]\n",
    "        print(f\"Computing hashes for {dataset_name} dataset ...\")\n",
    "        hashes = compute_file_hashes(dataset_path, dataset_name)\n",
    "        all_hashes.extend(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images hashed: 84882\n"
     ]
    }
   ],
   "source": [
    "hashes_df = pd.DataFrame(all_hashes)\n",
    "hashes_df.to_csv(\"../data/hashes/file_hashes_by_pair.csv\", index=False)\n",
    "print(f\"Total images hashed: {len(hashes_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hashes_for_pair(hashes_df, duplicate_dataset_name, original_dataset_name):\n",
    "    duplicate_df = hashes_df[hashes_df[\"Dataset\"] == duplicate_dataset_name].copy()\n",
    "    original_df = hashes_df[hashes_df[\"Dataset\"] == original_dataset_name].copy()\n",
    "    \n",
    "    duplicate_df.reset_index(drop=True, inplace=True)\n",
    "    original_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # merging on \"FileHash\" to find exact duplicates\n",
    "    duplicate_pairs = pd.merge(\n",
    "        duplicate_df,\n",
    "        original_df,\n",
    "        on=\"FileHash\",\n",
    "        suffixes=(\"_Duplicate\", \"_Original\")\n",
    "    )\n",
    "    \n",
    "    duplicate_pairs = duplicate_pairs.to_dict(\"records\")\n",
    "    \n",
    "    return duplicate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate pairs found between ISIC-2017-1-FOLD and ISIC-2017-Challenge: 1381\n",
      "\n",
      "Total duplicate pairs found between ISIC-2018 and ISIC-2018-Challenge: 3694\n",
      "\n",
      "Total duplicate pairs found between ISIC-2019-Preprocessed-Dataset and ISIC-2019-Challenge: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparing hashes for each dataset pair\n",
    "all_duplicate_pairs = []\n",
    "\n",
    "for pair in dataset_pairs:\n",
    "    duplicate_name = pair[\"Duplicate\"][\"Name\"]\n",
    "    original_name = pair[\"Original\"][\"Name\"]\n",
    "    \n",
    "    duplicate_pairs = compare_hashes_for_pair(hashes_df, duplicate_name, original_name)\n",
    "    \n",
    "    all_duplicate_pairs.extend(duplicate_pairs)\n",
    "    \n",
    "    duplicates_df = pd.DataFrame(duplicate_pairs)\n",
    "    csv_filename = f\"../data/hashes/duplicate_images_{duplicate_name}_vs_{original_name}.csv\"\n",
    "    duplicates_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Total duplicate pairs found between {duplicate_name} and {original_name}: {len(duplicates_df)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate pairs found across all dataset pairs: 5075\n"
     ]
    }
   ],
   "source": [
    "# all duplicate pairs across all dataset pairs\n",
    "all_duplicates_df = pd.DataFrame(all_duplicate_pairs)\n",
    "all_duplicates_df.to_csv(\"../data/hashes/all_duplicate_images_pairs.csv\", index=False)\n",
    "print(f\"Total duplicate pairs found across all dataset pairs: {len(all_duplicates_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
